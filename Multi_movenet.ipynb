{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.9.1 tensorflow-gpu==2.9.1 tensorflow-hub opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8339d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TF and TF Hub libraries.\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "# Download the model from TF Hub.\n",
    "model = hub.load(\"https://tfhub.dev/google/movenet/multipose/lightning/1\")\n",
    "movenet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2fd31954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keypoints_with_scores.shape = [6,17,3]\n",
    "#6 persons, 17 keypoints, 3 numbers = [y coor, x coord, confident]\n",
    "#keypoint = [nose, left eye, right eye, left ear, right ear, left shoulder, right shoulder, left elbow, right elbow, left wrist, right wrist, left hip, right hip, left knee, right knee, left ankle, right ankle]\n",
    "\n",
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 6, (0,255,0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76b214d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect key point\n",
    "\n",
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75918bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw arrow for head direction\n",
    "def draw_head_direction(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    confident_booster = 0.2\n",
    "    c1 = shaped[3][2] + confident_booster\n",
    "    c2 = shaped[4][2] + confident_booster\n",
    "    \n",
    "    if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "        middle_eartoear = (shaped[4] + shaped[3])/2\n",
    "        cv2.arrowedLine(frame, (int(middle_eartoear[1]), int(middle_eartoear[0])), (int(shaped[0][1]), int(shaped[0][0])), (0,0,255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42cda874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to loop through each person detected and render\n",
    "\n",
    "def loop_through_people(frame, keypoints_with_scores, edges, confidence_threshold):\n",
    "    for person in keypoints_with_scores:\n",
    "        #draw_connections(frame, person, edges, confidence_threshold)\n",
    "        #draw_keypoints(frame, person, confidence_threshold)\n",
    "        draw_head_direction(frame, person, edges, confidence_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09d92406",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('test001.mp4')\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Resize image\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 384,640)\n",
    "    input_img = tf.cast(img, dtype=tf.int32)\n",
    "    \n",
    "    # Detection section\n",
    "    results = movenet(input_img)\n",
    "    keypoints_with_scores = results['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "    \n",
    "    # Render keypoints \n",
    "    loop_through_people(frame, keypoints_with_scores, EDGES, 0.4)\n",
    "    \n",
    "    cv2.imshow('Movenet Multipose', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
